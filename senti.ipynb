{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import csv\n",
    "import textblob\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Lexicon Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dictionaries\n",
    "pos_dic=pd.read_csv('positive.csv')\n",
    "neg_dic=pd.read_csv('negative.csv')\n",
    "neu_dic=pd.read_csv('neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into one dataframe\n",
    "pos_dic['polarity']= 1\n",
    "neg_dic['polarity']= -1\n",
    "neu_dic['polarity']= 0\n",
    "pos_dic.rename(columns={'Positive':'Words'},inplace=True)\n",
    "neg_dic.rename(columns={'Negative':'Words'},inplace=True)\n",
    "neu_dic.rename(columns={'Neutral':'Words'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "diclist=[pos_dic,neg_dic,neu_dic]\n",
    "lexicon=pd.concat(diclist,keys=['Words','polarity'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_map = {\n",
    "         re.compile(\"rt [@0-9a-z_]{0,10}:\"),\n",
    "         re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"),\n",
    "         re.compile(\"#[0-9a-z]+\"),\n",
    "         re.compile(\"@[0-9a-z]+\"),\n",
    "    }\n",
    "    \n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "def lower_and_remove_with_reg(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        for v in reg_map:\n",
    "            text = v.sub(\"\", text)\n",
    "        return text\n",
    "    \n",
    "lexicon['cleaned']=lexicon['Words'].apply(lower_and_remove_with_reg).replace('[^a-zA-Z0-9 ]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buyfxe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>longfxe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buysignal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upsidebreakout</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eurusdbullintact</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>decreaseeurusdshort</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>holdeurusdshort</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cleaned  polarity\n",
       "0                  buyfxe         1\n",
       "1                 longfxe         1\n",
       "2               buysignal         1\n",
       "3          upsidebreakout         1\n",
       "4        eurusdbullintact         1\n",
       "..                    ...       ...\n",
       "146  currentlyshorteurusd        -1\n",
       "147   decreaseeurusdshort        -1\n",
       "148  currentlyshorteurusd        -1\n",
       "149       holdeurusdshort        -1\n",
       "150  currentlyshorteurusd        -1\n",
       "\n",
       "[151 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order=['cleaned','polarity']\n",
    "lexicon=lexicon[order]\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df) :\n",
    "    # 1. Convert text into lowercase\n",
    "    # 2. Remove non-alphabetic characters\n",
    "    reg_map = {\n",
    "         re.compile(\"rt [@0-9a-z_]{0,10}:\"),\n",
    "         re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"),\n",
    "         re.compile(\"#[0-9a-z]+\"),\n",
    "         re.compile(\"@[0-9a-z]+\"),\n",
    "    }\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    def lower_and_remove_with_reg(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        for v in reg_map:\n",
    "            text = v.sub(\"\", text)\n",
    "        return text\n",
    "    \n",
    "    df['cleaned']=df['tweet'].apply(lower_and_remove_with_reg).replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanner(text,lexicon):\n",
    "    text = re.sub('[^a-zA-Z]', '', str(text))\n",
    "    score=0\n",
    "    for i in lexicon:\n",
    "        count = len(re.findall(re.escape(i[0]), text))\n",
    "        score += count * i[1]\n",
    "\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d02aeee2ffdfbdf839e8f3843dba70a3a2400920afa710074e52a3ca82d748f8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
