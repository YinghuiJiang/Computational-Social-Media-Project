{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import csv\n",
    "import textblob\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Lexicon Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dictionaries\n",
    "pos_dic=pd.read_csv('positive.csv')\n",
    "neg_dic=pd.read_csv('negative.csv')\n",
    "neu_dic=pd.read_csv('neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into one dataframe\n",
    "pos_dic['polarity']= 1\n",
    "neg_dic['polarity']= -1\n",
    "neu_dic['polarity']= 0\n",
    "pos_dic.rename(columns={'Positive':'Words'},inplace=True)\n",
    "neg_dic.rename(columns={'Negative':'Words'},inplace=True)\n",
    "neu_dic.rename(columns={'Neutral':'Words'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diclist=[pos_dic,neg_dic,neu_dic]\n",
    "lexicon=pd.concat(diclist,keys=['Words','polarity'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_map = {\n",
    "         re.compile(\"rt [@0-9a-z_]{0,10}:\"),\n",
    "         re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"),\n",
    "         re.compile(\"#[0-9a-z]+\"),\n",
    "         re.compile(\"@[0-9a-z]+\"),\n",
    "    }\n",
    "    \n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "def lower_and_remove_with_reg(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        for v in reg_map:\n",
    "            text = v.sub(\"\", text)\n",
    "        return text\n",
    "    \n",
    "lexicon['cleaned']=lexicon['Words'].apply(lower_and_remove_with_reg).replace('[^a-zA-Z0-9 ]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buyfxe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>longfxe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buysignal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upsidebreakout</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eurusdbullintact</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>decreaseeurusdshort</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>holdeurusdshort</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>currentlyshorteurusd</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cleaned  polarity\n",
       "0                  buyfxe         1\n",
       "1                 longfxe         1\n",
       "2               buysignal         1\n",
       "3          upsidebreakout         1\n",
       "4        eurusdbullintact         1\n",
       "..                    ...       ...\n",
       "146  currentlyshorteurusd        -1\n",
       "147   decreaseeurusdshort        -1\n",
       "148  currentlyshorteurusd        -1\n",
       "149       holdeurusdshort        -1\n",
       "150  currentlyshorteurusd        -1\n",
       "\n",
       "[151 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order=['cleaned','polarity']\n",
    "lexicon=lexicon[order]\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df) :\n",
    "    # 1. Remove tweets from user zhanusic and Violahkzvo\n",
    "    # 2. Convert text into lowercase\n",
    "    # 3. Remove non-alphabetic characters\n",
    "\n",
    "    df = df.drop((df[df['author username'] == 'zhanusic'].index) | (df[df['author username'] == 'Violahkzvo']).index).reset_index(drop=True)\n",
    "\n",
    "    reg_map = {\n",
    "         re.compile(\"rt [@0-9a-z_]{0,10}:\"),\n",
    "         re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"),\n",
    "         re.compile(\"#[0-9a-z]+\"),\n",
    "         re.compile(\"@[0-9a-z]+\"),\n",
    "    }\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    def lower_and_remove_with_reg(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        for v in reg_map:\n",
    "            text = v.sub(\"\", text)\n",
    "        return text\n",
    "    \n",
    "    df['cleaned']=df['tweet'].apply(lower_and_remove_with_reg).replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanner(text,lexicon):\n",
    "    text = re.sub('[^a-zA-Z]', '', str(text))\n",
    "    score=0\n",
    "\n",
    "    for index in range(lexicon.shape[1]):\n",
    "        dict_word = lexicon.iloc[index]['cleaned']\n",
    "        count = len(re.findall(re.escape(dict_word), text))\n",
    "        polarity = lexicon.iloc[index]['polarity']\n",
    "        score += count * polarity\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tweets = pd.read_csv(\"data/data_2013-07-26_eur_usd.csv\")\n",
    "tweets_text = tweets['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tweets\n",
    "cleaned_tweets = preprocess(tweets)\n",
    "cleaned_text = cleaned_tweets['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cleaned_text.apply(lambda x: scanner(x, lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the sentiment analysis score for each tweet of all collected data. Then save the score with the corresponding tweet, group by the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames of the csv files\n",
    "csv_files = [x for x in os.listdir('data') if x.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scanner on all cleaned data, file by file. Then save the score for each tweet in each file in the senti_analysis_scores folder\n",
    "# The format for the scores file is e.g. scores_2011-09-26.csv\n",
    "for name in csv_files:\n",
    "  df = pd.read_csv('data/' + name)\n",
    "  cleaned_tweets = preprocess(df)\n",
    "  cleaned_text = cleaned_tweets['cleaned']\n",
    "\n",
    "  # Get the score for each tweet\n",
    "  scores = cleaned_text.apply(lambda x: scanner(x, lexicon))\n",
    "  score_df = pd.DataFrame({'tweet': cleaned_text, 'polarity_score': scores})\n",
    "\n",
    "  # Save file to senti_anaylsis _scores folder\n",
    "  score_filename = 'scores_'+ name[5:15] + '.csv'\n",
    "  score_df.to_csv('senti_analysis_scores/' + score_filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d02aeee2ffdfbdf839e8f3843dba70a3a2400920afa710074e52a3ca82d748f8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
